from collections import UserDict, UserList
import csv
from dataclasses import asdict, is_dataclass
import functools
import json
import logging
from pathlib import Path
import sys
from typing import Any, Dict, Generator, Iterable, List, Sequence, Tuple, Type, Union

from ..ioTypes.combination import Combination
from ..ioTypes.pelmo import PECResult, PelmoResult
from ..util.conversions import EnhancedJSONEncoder
from ..ioTypes.compound import Compound
from ..ioTypes.gap import GAP


def rebuild_scattered_to_file(file: Path, parent: Path, input_directories: Sequence[Path],
                              glob_pattern: str = "output.json", psm_root=Path.cwd()):
    write_results_to_file(rebuild_scattered_output(parent, input_directories, glob_pattern, psm_root), file)


def rebuild_output_to_file(file: Path, results: Union[Path, Iterable[PelmoResult]], input_directories: Sequence[Path],
                           psm_root=Path.cwd()):
    write_results_to_file(rebuild_output(results, input_directories, psm_root), file)


def write_results_to_file(results: Iterable[PECResult], file: Path):
    output_format = file.suffix[1:]
    if output_format == 'json':
        with file.with_suffix('.json').open('w') as fp:
            results = list(results)
            json.dump(results, fp, cls=EnhancedJSONEncoder)
    elif output_format == 'csv':
        with file.with_suffix('.csv').open('w', newline='') as fp:
            writer = csv.writer(fp, )
            # noinspection PyProtectedMember,PyTypeChecker
            doubles = list(flatten_to_tuples(next(results)._asdict()))
            header = [x[0] for x in doubles]
            row = [x[1] for x in doubles]
            writer.writerow(header)
            writer.writerow(row)
            # noinspection PyProtectedMember
            writer.writerows((x[1] for x in flatten_to_tuples(r._asdict())) for r in results)
    else:
        raise ValueError("Could not infer format, please specify explicitly")


def flatten_to_tuples(o: Any, prefix: List[str] = []) -> Generator[Tuple[str, str], None, None]:
    if isinstance(o, (dict, UserDict)):
        yield from flatten_dict_to_tuples(o, prefix)
    elif isinstance(o, (list, UserList, tuple)):
        yield from flatten_list_to_tuples(o, prefix)
    elif is_dataclass(o):
        for key, value in flatten_to_tuples(asdict(o), prefix):
            yield key, value
    else:
        yield ".".join(prefix), str(o)


def flatten_dict_to_tuples(d: Dict, prefix: List[str] = []) -> Generator[Tuple[str, str], None, None]:
    sys.stdout.flush()
    for key, value in d.items():
        for k, v in flatten_to_tuples(value, prefix=prefix + [str(key)]):
            yield k, v


def flatten_list_to_tuples(to_flatten: List, prefix: List[str] = []) -> Generator[Tuple[str, str], None, None]:
    for index, value in enumerate(to_flatten):
        for k, v in flatten_to_tuples(value, prefix=prefix + [str(index)]):
            yield k, v


def rebuild_scattered_output(parent: Path, input_directories: Sequence[Path], glob_pattern: str = "output.json",
                             psm_root=Path.cwd()) -> Generator[PECResult, None, None]:
    logger = logging.getLogger()
    logger.debug("Iterating over output files %s", list(parent.rglob(glob_pattern)))
    for file in parent.rglob(glob_pattern):
        yield from rebuild_output(file, input_directories, psm_root)


def rebuild_output(source: Union[Path, Iterable[PelmoResult]], input_directories: Sequence[Path],
                   psm_root=Path.cwd()) -> Generator[PECResult, None, None]:
    logger = logging.getLogger()
    if isinstance(source, Path):
        with source.open() as fp:
            outputs = json.load(fp)
        outputs = [PelmoResult(**item) for item in outputs]
    else:
        outputs = source
    for output in outputs:
        psm_file = psm_root / output.psm
        with psm_file.open() as psm:
            psm.readline()
            input_data_hashes = json.loads(psm.readline())
        if 'compound' in input_data_hashes.keys() and 'gap' in input_data_hashes.keys():
            compound_hash = input_data_hashes['compound']
            gap_hash = input_data_hashes['gap']
            logger.debug({"compound_file": compound_hash, "gap_file": gap_hash})
            compound = get_obj_by_hash(h=compound_hash, file_roots=input_directories)
            gap = get_obj_by_hash(h=gap_hash, file_roots=input_directories)
        elif 'combination' in input_data_hashes.keys():
            combination_hash = input_data_hashes['combination']
            combination = get_obj_by_hash(h=combination_hash, file_roots=input_directories)
            compound = combination.compound
            gap = combination.gap
        else:
            raise ValueError('Could not find origin input data for psm file. '
                             'Is the comment set to the metadata generated by creator.py?')
        yield PECResult(compound=compound, gap=gap, crop=output.crop, scenario=output.scenario, pec=output.pec)


@functools.lru_cache(maxsize=None)
def get_obj_by_hash(h: int, file_roots: Sequence[Path]) -> Union[Compound, GAP, Combination]:
    hashes = {}
    for file_root in file_roots:
        hashes.update(get_hash_obj_relation(file_root, (Compound, GAP, Combination)))
    return hashes[h]


@functools.lru_cache(maxsize=None)
def get_hash_obj_relation(directory: Path, candidate_classes: Tuple[Type, ...]) -> Dict[int, Any]:
    hashes = {}
    if directory.is_dir():
        files = directory.glob('*.json')
    else:
        files = [directory]
    for file in files:
        for candidate in candidate_classes:
            try:
                with file.open() as fp:
                    obj = candidate(**json.load(fp))
                    hashes[hash(obj)] = obj
            except TypeError:
                pass
    return hashes
