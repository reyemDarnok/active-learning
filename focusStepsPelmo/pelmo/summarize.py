import csv
import functools
import json
import logging
from pathlib import Path
from typing import Any, Dict, Generator, Iterable, Sequence, Tuple, Type, Union

from focusStepsPelmo.ioTypes.combination import Combination
from focusStepsPelmo.ioTypes.compound import Compound
from focusStepsPelmo.ioTypes.gap import GAP
from focusStepsPelmo.ioTypes.pelmo import PECResult, PelmoResult
from focusStepsPelmo.util.conversions import EnhancedJSONEncoder
from focusStepsPelmo.util.datastructures import correct_type


def rebuild_scattered_to_file(file: Path, parent: Path, input_directories: Tuple[Path, ...],
                              glob_pattern: str = "output.json"):
    write_results_to_file(rebuild_scattered_output(parent, input_directories, glob_pattern), file)


def rebuild_output_to_file(file: Path,
                           results: Union[Path, Iterable[PelmoResult]], input_directories: Tuple[Path, ...]):
    write_results_to_file(rebuild_output(results, input_directories), file)


def write_results_to_file(results: Iterable[PECResult], file: Path):
    output_format = file.suffix[1:]
    file.parent.mkdir(exist_ok=True, parents=True)
    if output_format == 'json':
        with file.with_suffix('.json').open('w') as fp:
            results = list(results)
            json.dump(results, fp, cls=EnhancedJSONEncoder)
    elif output_format == 'csv':
        with file.with_suffix('.csv').open('w', newline='') as fp:
            writer = csv.writer(fp, )
            results_iter = iter(results)
            first_result: PECResult = next(results_iter)
            writer.writerow(first_result.get_csv_headers())
            writer.writerow(first_result.to_list())
            writer.writerows(r.to_list() for r in results_iter)
    else:
        raise ValueError("Could not infer format, please specify explicitly")



def rebuild_scattered_output(parent: Path, input_directories: Tuple[Path, ...], glob_pattern: str = "output.json",
                             ) -> Generator[PECResult, None, None]:
    logger = logging.getLogger()
    logger.debug("Iterating over output files %s", list(parent.rglob(glob_pattern)))
    for file in parent.rglob(glob_pattern):
        yield from rebuild_output(file, input_directories)


def rebuild_output(source: Union[Path, Iterable[PelmoResult]], input_directories: Tuple[Path, ...]
                   ) -> Generator[PECResult, None, None]:
    logger = logging.getLogger()
    if isinstance(source, Path):
        with source.open() as fp:
            outputs = json.load(fp)
        outputs = [PelmoResult(**item) for item in outputs]
    else:
        outputs = source
    for output in outputs:
        input_data_hashes = json.loads(output.psm_comment)
        if 'compound' in input_data_hashes.keys() and 'gap' in input_data_hashes.keys():
            compound_hash = input_data_hashes['compound']
            gap_hash = input_data_hashes['gap']
            logger.debug({"compound_file": compound_hash, "gap_file": gap_hash})
            compound = get_obj_by_hash(h=compound_hash, file_roots=input_directories)
            gap = get_obj_by_hash(h=gap_hash, file_roots=input_directories)
        elif 'combination' in input_data_hashes.keys():
            combination_hash = input_data_hashes['combination']
            combination = get_obj_by_hash(h=combination_hash, file_roots=input_directories)
            compound = combination.compound
            gap = combination.gap
        else:
            raise ValueError('Could not find origin input data for psm file. '
                             'Is the comment set to the metadata generated by creator.py?')
        pecs = {}
        if compound.metabolites:
            all_metabolites = {desc.metabolite for desc in compound.metabolites}
        else:
            all_metabolites = set()
        all_metabolites = all_metabolites.union(
            {desc.metabolite for metabolite in all_metabolites for desc in metabolite.metabolites})
        for compound_name, pec in output.pec.items():
            if compound_name == "parent":
                pecs[compound.name] = pec
            else:
                for metabolite in all_metabolites:
                    if 'pelmo' in metabolite.model_specific_data.keys():
                        if compound_name.casefold() == metabolite.model_specific_data['pelmo']['position'].casefold():
                            pecs[metabolite.name] = pec
                            break
                else:  # if for wasn't completed by break
                    first_letter = compound_name[0].upper()
                    order = compound_name[1]
                    metabolite = compound.metabolites[ord(first_letter) - ord('A')].metabolite
                    if order == "2":
                        metabolite = metabolite.metabolites[0].metabolite
                    pecs[metabolite.name] = pec

        yield PECResult(compound=compound, gap=gap, scenario=output.scenario, pec=pecs)


@functools.lru_cache(maxsize=None)
def get_obj_by_hash(h: int, file_roots: Sequence[Path]) -> Union[Compound, GAP, Combination]:
    hashes = {}
    for file_root in file_roots:
        hashes.update(get_hash_obj_relation(file_root, (Compound, GAP, Combination)))
    return hashes[h]


@functools.lru_cache(maxsize=None)
def get_hash_obj_relation(directory: Path, candidate_classes: Tuple[Type, ...]) -> Dict[int, Any]:
    hashes = {}
    if directory.is_dir():
        files = directory.glob('*.json')
    elif directory.exists():
        files = [directory]
    else:
        files = []
    from_file_candidates = {candidate for candidate in candidate_classes if hasattr(candidate, 'from_file')}
    json_candidates = {candidate for candidate in candidate_classes if candidate not in from_file_candidates}
    for file in files:
        for candidate in from_file_candidates:
            objs = candidate.from_file(file)
            try:
                for obj in objs:
                    hashes[hash(obj)] = obj
            except TypeError:
                continue
        for candidate in json_candidates:
            with file.open() as fp:
                json_data = json.load(fp)
            # noinspection PyBroadException
            try:
                # noinspection PyTypeChecker
                obj = correct_type(json_data, candidate)
            except TypeError:
                # Failure of conversion is expected to frequently happen
                continue
            hashes[hash(obj)] = obj

    return hashes
